
%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%
%\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
%\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
%\smartqed  % flush right qed marks, e.g. at end of proof
%
%\usepackage{graphicx}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%

%% ===============================================


%\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
%\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
%\smartqed  

\documentclass[11pt]{article}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{subfig, multicol}
\usepackage[all]{xy}
\usepackage[round]{natbib}
\usepackage{url}
%\usepackage[pdftex]{graphicx}
\usepackage[T1]{fontenc}
\usepackage{amsfonts}
%\usepackage{amsthm}
\usepackage{dsfont}
\usepackage{arydshln}


%% MINE

% \setlength{\parindent}{0pt}

\newcommand{\fixme}[1]{\textsl{[#1]}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argtop}{arg\,top}

%\newtheorem{definition}{Definition}
%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}{Lemma}

\begin{document}

\title{A Finnish Corpus of Historical Text for Named Entity Recognition}
\author{Kimmo \and Teemu  \and Mika}

\maketitle


\begin{abstract}
\noindent We present a corpus of historical Finnish text with a manually prepared named entity annotation. The corpus consists of  X word tokens with three named entity classes (organization, location, person) and is freely available for research purposes. In experiments employing the Stanford Named Entity Recognizer, the system yieids an overall F1-score of X. 

\end{abstract}

\section{Introduction}
\label{sec: introduction}


The rest of the paper is organized as follows. We first describe the corpus in Section \ref{sec: corpus}. We then present experimental results on the corpus using the Stanford Named Entity Recognizer in Section \ref{sec: experiments}. Finally, conclusions on the work are presented in Section \ref{sec: conclusions}.




\section{Corpus}
\label{sec: corpus}

\subsection{Text}
\label{sec: text}

The text material is extracted from the archives of Digitoday, a Finnish online technology news source.\footnote{See \url{www.digitoday.fi}.} The material covers a variety of technology related topics, such as business, science, and information security.  The material is extracted from articles published between years 2014 and 2015. The extracted text contains 1,194 articles (260,637 word tokens) in total. 
%In comparison, the English and German sections of the CoNLL-2003 data set \citep{tjong2003} contain X and Y word tokens in total, respectively. 
In addition to raw word forms, the corpus includes meta data describing the publishing date of the articles and if the word tokens belong to a headline, an ingress, or an article body. 


\subsection{Named Entities}
\label{sec: named entities}

The set of named entity classes contains the three fundamental entity types, \textit{person}, \textit{organization}, \textit{location}, collectively referred to as the \textit{enamex} since MUC-6 competition \citep{grishman1996}. These three classes appear in the vast majority of published work on NER \citep{nadeau2007}. In addition, our annotation contains the classes \textit{products}, \textit{events}, \textit{dates}, and \textit{person titles} which have been employed in previous work to varying extent. In the following, we discuss these classes in more detail.
%In the following, we describe the classes in more detail.

%While we do not consider the last two named entities, we will refer to them as such for simplicity. 

\paragraph{Person (PER)}

Markable person names include:

\begin{itemize}

\item[]

\end{itemize}


\paragraph{Location (LOC)}

Markable locations include:

\begin{itemize}

\item[]

\end{itemize}


\paragraph{Organization (ORG)}

Markable organizations include:

\begin{itemize}

\item[]

\end{itemize}


\subsection{General Annotation Guidelines}

In the following, we address some general aspects of the annotation.

\paragraph{Multi-Word Entities and Nesting}

The marked entities can span multiple word tokens, for example, consider "Sauli Niinist\"o" or ''Nokia Solutions and Networks''. As for multi-token entities, it is in general possible to employ either a \textit{nested} (overlapping) or \textit{non-nested} (non-overlapping) annotation approach. In the nested case, an expression such as ''Manchester United'' would be marked as an organization while its sub-part ''Manchester'' would be marked as a location.  In our annotation, however, we follow the non-nested annotation approach, that is, only the longer span is considered markable. In previous work, this has been the prevalent annotation strategy, apart from a few expections \citep{byrne2007,benikova2014}.

\paragraph{Derivation and Compounding}

To be markable, each class must appear as a whole and not as a derivation or as a part of a token. For example, ''Suomi'' (Finland) is considered as a markable location, whereas ''suomalainen'' (Finnish) and ''Suomi-fani'' (a fan of Finland) are not.  An important exception to this rule is the set of cases, in which a markable named entity forms a part of a compound word while the whole compound word refers to that same entity: for example, consider the expression ''Google-ohjelmistoyhti\"o'' (the software company Google) instead of  ''Google'' or ''iPhone-puhelin'' (iPhone phone) instead of ''iPhone''. In these cases, the compound word is considered markable.

\paragraph{Coordination}

Written Finnish regularly employs a type of coordination between compound words which share a common part. For example, the expression ''Windows-k\"aytt\"oj\"arjestelm\"a ja Linux-k\"aytt\"oj\"arjestelm\"a'' (Windows operating system and Linux operating system) is written in a more concise manner as ''Windows- ja Linux-k\"aytt\"oj\"arjestelm\"at'' (Windows and Linux operating systems). In these cases, the tokens ''Windows-" and ''Linux-k\"aytt\"oj\"arjestelm\"at'' are both considered markable. This rule generalizes to multiple tokens as well: for example, consider ''\textbf{Windows-}, \textbf{Linux}- ja \textbf{OSX-k\"aytt\"oj\"arjestelm\"at}''.

Another coordination issue addresses such cases as ''Windows 8 ja 10'' (Windows 8 and 10) or ''iPhone 6 sek\"a 6s Plus'' (iPhone 6 as well as 6s Plus). We consider these cases to be markable as single expressions. %The justification for this choice is as follows: if the expression ''Windows 8 ja 10'' would be divided into two entities, ''Windows 8'' and ''10'', the connection between ''Windows'' and ''10'' would be lost. Finally, the rule also generalizes to multiple tokens: for example, consider 'Windows XP, Vista ja 10''.
This rule also generalizes to multiple tokens: for example, consider '\textbf{Windows XP, Vista ja 10}''.

\paragraph{Expression \textit{-niminen}}
	
Written Finnish regularly employs a type of expression, in which a name is associated with a noun using a suffix ''-niminen'': for example, consider ''Sauli-niminen henkil\"o'' (a person named Sauli) or ''iPhone-niminen puhelin'' (a phone named iPhone). In these cases, we consider the complete expression markable, that is, 'Sauli-niminen henkil\"o'' and ''iPhone-niminen puhelin'' would be marked as single \textit{person} and \textit{product} entities, respectively. We generalize this rule to also cover cases ''-merkkinen'' (of mark) and ''-mallinen'' (of model): for example, consider ''Tesla-merkkinen auto'' (a car of the brand Tesla) and ''Samsung-mallinen puhelin'' (a phone of the model Samsung). 


\subsection{Annotation Process}

%In order to annotate the text described in Section \ref{sec: text} using the named entities described in Section \ref{sec: named entities}, we first divide the text into two parts, training and test sections. In the training section, we include all articles published during 2014, while the test section consists of articles published in 2015. The resulting training and test sections contain xx and yy word tokens, respectively. Subsequently, the training section is annotated by a single annotator (Annotator A). Meanwhile, annotation of the test section is carried out by three annotators (Annotator A, Annotator B, and Annotator C). Specifically, the section is first annotated independently by Annotators A and B. Subsequently, found conflicts are resolved using the majority vote principle by Annotator C. The purpose of this three-stage procedure is to ensure a high test set quality.

The annotation process was performed iteratively by in total four annotators. In other words, the annotation and annotation guideline were defined and improved during multiple passes over the data set. The annotation was performed over a span of one year.



% Specifically, the section is first annotated by Annotator A. Subsequently, the annotation is proof read by Annotator B. Found conflicts are then resolved using the majority vote principle by all Annotators A, B, and C.


\subsection{Annotation Statistics}
\label{sec: annotation statistics}

%The complete data set consists of X articles (X word tokens). In what follows, we will separate the data set into two non-overlapping sections, training and test sets.  In the training set, we include all articles published during 2014, while the test set consists of articles published in 2015. The resulting training and test sections contain 953 and 241 articles (xx and yy word tokens), respectively. The counts of each named entity class in these sections are presented in Table \ref{tab: statistics}.
 
The complete data set consists of 1,194 articles (260,637 word tokens). The counts of each named entity class in these sections are presented in Table \ref{tab: statistics}.

\begin{table}[h!]
%\begin{small}
\begin{center}
\begin{tabular}{lcc} 
\hline
\noalign{\smallskip}
class & count & \% \\ 
\hline
\noalign{\smallskip}
ORG & &  \\
PER & &  \\
LOC &  &   \\
\hline
\noalign{\smallskip}
TOTAL &  &  \\
\end{tabular}
\end{center}
% \caption{Statistics of the training and test sets including total number of tokens and named entity classes.}
\caption{Counts and relative portions of named entity classes.}
\label{tab: statistics} 
\end{table}

        

\subsection{Gazetteers}
\label{sec: gazetteers}


In addition to the manually prepared named entity annotation, our corpus includes a set of gazetteers which map words into categories. For example, ''Samsung'' is mapped into the category ''yritys'' (''company'') and''Nokia'' into the categories''yritys'' and ''Suomen kunta'' (''Finnish town''). The categories are extracted from the info boxes of Finnish Wikipedia articles in an automatic manner.

\fixme{more information}

\subsection{File Format}

The data is presented in a two-column file format using the standard BIO notation with an empty line separating sentences. For example, consider the following sentence ''Applen Tim Cook myy iPhoneja suomalaisille.'' (''Apple's Tim Cook sells iPhones to the Finnish.''):
%\begin{table}[h!]
%\begin{small}
\begin{center}
\begin{tabular}{lc} 
%\hline
%\noalign{\smallskip}
Applen & B-ORG  \\      
Tim & B-PER \\
Cook & I-PER \\
myy & O \\
iPhoneja & B-PRO \\      
suomalaisille & O \\
. & O \\
%\hline
%\noalign{\smallskip}
\end{tabular}
\end{center}
% \caption{Statistics of the training and test sets including total number of tokens and named entity classes.}
%\caption{Counts and relative portions of named entity classes.}
%\label{tab: statistics} 
%\end{table}


%\subsection{Morphological Annotation}

%In addition to the manually prepared named entity annotation, we augment the corpus with automatically assigned lemmas and morphological labels. To this end, we utilize the FinnPos toolkit \citep{silfverberg2016}, a recently published morphological and lemmatization toolkit for Finnish. FinnPos contains two separate tagging and lemmatization models learned from Finnish Turku Dependency Treebank (TDT) \citep{haverinen2014} and FinnTreeBank (FTB) \citep{voutilainen2011}. Thus, each word token in the corpus is tagged and lemmatized by FinnPos using the two models.





% Specifically, the section is first annotated independently by Annotators A and B. Subsequently Subsequently, the annotations are compared by Annotator C. In case of conflicts

%\subsection{Comments and Known Issues}

%\paragraph{Organization versus Product} 1) For example, consider the two related but distinct meanings of Youtube in "Katsoa videoita Youtubesta" ja "Youtube on Googlen omistama yhti\"o". These are disambiguated according to context. 2) Websites are most often used in organisation-sense (Amazon.com, Verkkokauppa.com) but can be also used in product-sense ("Amazon.com-sivusto kaatui keskiy\"oll\"a."). The product-sense is chosen if appropriate.

%\paragraph{Organization versus Location} For example, consider India as a geographical location in "" and India as a governmental organization in "".

%\paragraph{Explicit naming} For example, consider "Nokia N1-niminen puhelin".





\section{Experiments}
\label{sec: experiments}

In this section, we present experimental results on the corpus employing the Stanford Named Entity Recognizer toolkit \citep{finkel2005}. In what follows, we will first describe the utilized training and test set splits in Section \ref{sec: data}. We then discuss the Stanford Named Entity Recognizer in Section \ref{sec: stanford named entity recognizer} and the employed evaluation measures in Section \ref{sec: evaluation}. Finally, we present and discuss the obtained results in Section \ref{sec: results}.


\subsection{Data}
\label{sec: data}

As presented in Section \ref{sec: annotation statistics},  the complete corpus consists of X word tokens. In order to carry out the experiments, we separate the data into two non-overlapping sections, training and test sets.  In the training set, we include all articles published during 2014, while the test set consists of articles published in 2015. The resulting training and test sections contain X and Y word tokens, respectively. In addition, we form a separate development set from the training data by extracting every 10th article, starting from the 10th article. (The articles are separated using the headline information referred to in Section \ref{sec: corpus}.) %The resulting training, development, and test set sizes are presented in 


\subsection{Stanford Named Entity Recognizer}
\label{sec: stanford named entity recognizer}

The Stanford Named Entity Recognizer \citep{finkel2005} is a freely available NER toolkit based on machine learning methodology.\footnote{The toolkit is available at \url{http://nlp.stanford.edu/software/CRF-NER.shtml}.} Given an annotated training data set and a feature extraction scheme specification, the toolkit can be employed to learn new NER models. Specifically, the toolkit contains an implementation of an arbitrary order conditional random field (CRF) model \citep{lafferty2001,finkel2005}. We use a wide range of substring, word context and orthography features. In addition, we use the gazetteers described in Section \ref{sec: gazetteers} as well as a gazetteer containing the lemmas and morphological tags yielded by the FinnPos tagger.\footnote{See the configuration file} The morphological tagger FinnPos contains two separate tagging and lemmatization models learned from Finnish Turku Dependency Treebank (TDT) \citep{haverinen2014} and FinnTreeBank (FTB) \citep{voutilainen2011}, of which we employ the latter. 



\subsection{Evaluation Measures}
\label{sec: evaluation}

We evaluate the system using the standard \textbf{precision} (the number of correctly recognized entities divided by the number of all recognized entities), \textbf{recall} (the number of correctly recognized entities divided by the the number of all annotated entities in data), and \textbf{F1-score} (the harmonic mean of precision and recall). %More formally, precision, recall, and F-score (for each named entity class separately) are defined as follows:
%\begin{equation}
%\text{precision} = \frac{\text{number of correctly recognized entities}}{y}
%\end{equation}
%\begin{equation}
%\text{recall} = \frac{x}{y} \\
%\end{equation}
%\begin{equation}
%\text{F-score} = \frac{x}{y} \\
%\end{equation}

%In order to gain more insight on the type of errors produced by the system, we analyze a \textbf{confusion matrix} of entities. In the matrix, we consider all entities which are located correctly, i.e., have a correct span over word tokens, by the system. The elements of the matrix then indicate which entity-spans were correctly detected but then assigned an incorrect entity-label. 

%Due to the rich morphology of Finnish, we are also interested in examining the effect of \textbf{out-of-vocabulary} (\textbf{OOV}) words on the recognition performance. To this end, we study the precision, recall, and F-scores obtained for entities in test set which include one or more OOV word form. A word form is considered OOV if it does not appear in the training data set.




\subsection{Results}
\label{sec: results}

Obtained results are presented in Table \ref{tab: precision recall and f-scores}. The system yielded an overall F1-score of X with precision and recall scores of  X and Y, respectively.

\begin{table}[t!]
%\begin{small}
\begin{center}
\begin{tabular}{l|ccc}
 class & precision & recall & F1\\
\hline
ORG &  85.93 & 86.43 & 86.18\\
PRO &  73.82 & 69.41 & 71.55\\
PER &  70.09 & 75.62 & 72.75\\
LOC &  86.02 & 89.13 & 87.55\\
DATE &  88.06 & 86.69 & 87.37\\
TITLE &  91.13 & 87.60 & 89.33\\
EVENT &  100.00 & 41.18 & 58.33\\
\hline
ALL & 82.77 & 82.08 & 82.42
\end{tabular}
\end{center}
%\end{small}
\caption{Precision, recall, and F-scores for each named entity class.}
\label{tab: precision recall and f-scores}
\end{table}




\section{Conclusions}
\label{sec: conclusions}

In this paper, we described a new corpus consisting of Finnish news articles with a manually prepared named entity annotation. The corpus contains X word tokens annotated using a set of three named entity classes, namely,  \textit{persons}, \textit{organizations}, and \textit{locations}. The corpus is freely available. In addition to description of the data, we presented experimental results on the corpus employing the Stanford Named Entity recognizer. The resulting system yielded a total F1-score of X.



%%%%%%%%%%% The bibliography starts:
\newpage
\bibliographystyle{plainnat}
%\bibliographystyle{spbasic}
\bibliography{finer}









\end{document}


